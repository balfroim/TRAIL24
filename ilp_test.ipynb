{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.csv_loader import CSVLoader\n",
    "from models.products.product_registry import ProductRegistry\n",
    "from models.products.product_mapping_row import ProductMappingRow\n",
    "from models.products.product_row import ProductRow\n",
    "\n",
    "product_registry = ProductRegistry(CSVLoader(ProductRow).read(), CSVLoader(ProductMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.users.user_registry import UserRegistry\n",
    "from models.users.user_mapping_row import UserMappingRow\n",
    "from models.users.user_row import UserRow\n",
    "\n",
    "user_registry = UserRegistry(CSVLoader(UserRow).read(), CSVLoader(UserMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ratings.rating_registry import RatingRegistry\n",
    "from models.ratings.rating_row import RatingRow\n",
    "\n",
    "rating_registry = RatingRegistry(CSVLoader(RatingRow).read(), user_registry, product_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userid gender       age\n",
      "0       1      F  Under 18\n",
      "1       2      M       56+\n",
      "2       3      M     25-34\n",
      "3       4      M     45-49\n",
      "4       5      M     25-34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extracting attributes from the User objects\n",
    "user_data = [\n",
    "    {\n",
    "        \"userid\": user.uid,\n",
    "        \"gender\": user.gender,\n",
    "        \"age\": user.age,\n",
    "    }\n",
    "    for user in user_registry.users\n",
    "]\n",
    "\n",
    "# Converting to DataFrame\n",
    "users = pd.DataFrame(user_data)\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "#print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_registry.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>productid</th>\n",
       "      <th>pname</th>\n",
       "      <th>pgenre</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>1193</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>661</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>914</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>3408</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>Under 18</td>\n",
       "      <td>2355</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid gender       age  productid                                   pname  \\\n",
       "0       1      F  Under 18       1193  One Flew Over the Cuckoo's Nest (1975)   \n",
       "1       1      F  Under 18        661        James and the Giant Peach (1996)   \n",
       "2       1      F  Under 18        914                     My Fair Lady (1964)   \n",
       "3       1      F  Under 18       3408                  Erin Brockovich (2000)   \n",
       "4       1      F  Under 18       2355                    Bug's Life, A (1998)   \n",
       "\n",
       "      pgenre  rating  timestamp  \n",
       "0      Drama       5  978300760  \n",
       "1  Animation       3  978302109  \n",
       "2    Musical       3  978301968  \n",
       "3      Drama       4  978300275  \n",
       "4  Animation       5  978824291  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting attributes from the Rating objects\n",
    "ratings_data = [\n",
    "    {\n",
    "        \"userid\": rating.user.uid,\n",
    "        \"gender\": rating.user.gender,\n",
    "        \"age\": rating.user.age,\n",
    "        \"productid\": rating.product.pid,\n",
    "        \"pname\": rating.product.name,\n",
    "        \"pgenre\": rating.product.genre,\n",
    "        \"rating\": rating.rating,\n",
    "        \"timestamp\": rating.timestamp\n",
    "    }\n",
    "    for rating in rating_registry.ratings\n",
    "]\n",
    "\n",
    "# Converting to DataFrame\n",
    "ratings = pd.DataFrame(ratings_data)\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully saved to ratings_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_file = \"ratings_dataset.xlsx\"\n",
    "ratings.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Dataset successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932293, 8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset successfully saved to ratings_sample_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "sample_df = ratings.sample(n=118, random_state=42)\n",
    "\n",
    "# Saving the sample DataFrame to an Excel file\n",
    "output_file = \"ratings_sample_dataset.xlsx\"\n",
    "sample_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Sample dataset successfully saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_1.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_2.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_3.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_4.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_5.pl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_balanced_partitions(file_path, output_dir):\n",
    "    # Load the dataset\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Define all possible categories for age and genres\n",
    "    possible_ages = {\n",
    "        \"Under 18\": \"under_18\",\n",
    "        \"18-24\": \"b18to24\",\n",
    "        \"25-34\": \"b25to34\",\n",
    "        \"35-44\": \"b35to44\",\n",
    "        \"45-49\": \"b45to49\",\n",
    "        \"50-55\": \"b50to55\",\n",
    "        \"56+\": \"plus56\"\n",
    "    }\n",
    "    \n",
    "    possible_genres = {\n",
    "        \"Action\": \"action\",\n",
    "        \"Adventure\": \"adventure\",\n",
    "        \"Animation\": \"animation\",\n",
    "        \"Comedy\": \"comedy\",\n",
    "        \"Drama\": \"drama\",\n",
    "        \"Sci-Fi\": \"sci_fi\",\n",
    "        \"Romance\": \"romance\",\n",
    "        \"Musical\": \"musical\"\n",
    "    }\n",
    "\n",
    "    # Split the data into positive and negative examples\n",
    "    positive_data = data[data['rating'] > 3].copy()\n",
    "    negative_data = data[data['rating'] <= 3].copy()\n",
    "\n",
    "    # Shuffle the data\n",
    "    positive_data = positive_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    negative_data = negative_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Determine the size of each partition\n",
    "    num_partitions = 5\n",
    "    pos_partition_size = len(positive_data) // num_partitions\n",
    "    neg_partition_size = len(negative_data) // num_partitions\n",
    "\n",
    "    # Create partitions\n",
    "    partitions = []\n",
    "    for i in range(num_partitions):\n",
    "        pos_start = i * pos_partition_size\n",
    "        pos_end = (i + 1) * pos_partition_size\n",
    "        neg_start = i * neg_partition_size\n",
    "        neg_end = (i + 1) * neg_partition_size\n",
    "\n",
    "        partition = pd.concat([\n",
    "            positive_data.iloc[pos_start:pos_end],\n",
    "            negative_data.iloc[neg_start:neg_end]\n",
    "        ]).reset_index(drop=True)\n",
    "        \n",
    "        partitions.append(partition)\n",
    "\n",
    "    # Generate Prolog programs for each partition\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for i, partition in enumerate(partitions):\n",
    "        output_file_name = f\"{output_dir}/prolog_program_partition_{i+1}\"\n",
    "        generate_progol_program_for_partition(partition, possible_ages, possible_genres, output_file_name)\n",
    "\n",
    "def generate_progol_program_for_partition(partition, possible_ages, possible_genres, output_file_name):\n",
    "    # Prepare containers for Prolog facts\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "    background_knowledge = set()\n",
    "\n",
    "    # Process each row in the partition\n",
    "    for _, row in partition.iterrows():\n",
    "        user_id = f\"u{row['userid']}\"\n",
    "        movie_id = f\"m{row['productid']}\"\n",
    "        rating = row['rating']\n",
    "        age_group = possible_ages.get(row['age'], \"\").lower()  # Replace age group with corresponding Prolog predicate\n",
    "        gender = row['gender'].lower()  # Normalize gender\n",
    "        genre = possible_genres.get(row['pgenre'], \"\").lower()  # Replace genre with corresponding Prolog predicate\n",
    "\n",
    "        # Generate positive and negative examples\n",
    "        if rating > 3:\n",
    "            positive_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "        else:\n",
    "            negative_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "\n",
    "        # Background knowledge: user attributes\n",
    "        if age_group:\n",
    "            background_knowledge.add(f\"{age_group}({user_id}).\")\n",
    "        background_knowledge.add(f\"{gender}({user_id}).\")\n",
    "\n",
    "        # Background knowledge: movie genre\n",
    "        if genre:\n",
    "            background_knowledge.add(f\"{genre}({movie_id}).\")\n",
    "\n",
    "    # Mode declarations\n",
    "    modeh_declaration = \"modeh(*, recommend(+user, +movie)).\"\n",
    "    modeb_declarations = [\n",
    "        f\"modeb(*, {age}(+user)).\" for age in possible_ages.values()\n",
    "    ] + [\n",
    "        \"modeb(*, m(+user)).\",\n",
    "        \"modeb(*, f(+user)).\"\n",
    "    ] + [\n",
    "        f\"modeb(*, {genre}(+movie)).\" for genre in possible_genres.values()\n",
    "    ]\n",
    "\n",
    "    # Determinations\n",
    "    determinations = [\n",
    "        f\"determination(recommend/2, {age}/1).\" for age in possible_ages.values()\n",
    "    ] + [\n",
    "        \"determination(recommend/2, m/1).\",\n",
    "        \"determination(recommend/2, f/1).\"\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, {genre}/1).\" for genre in possible_genres.values()\n",
    "    ]\n",
    "\n",
    "    # Combine all parts into a Progol-compatible logic program\n",
    "    progol_program = \"% Mode Declarations\\n\"\n",
    "    progol_program += modeh_declaration + \"\\n\"\n",
    "    progol_program += \"\\n\".join(modeb_declarations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Determinations\\n\"\n",
    "    progol_program += \"\\n\".join(determinations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Background Knowledge\\n:- begin_bg.\\n\"\n",
    "    progol_program += \"\\n\".join(sorted(background_knowledge)) + \"\\n:- end_bg.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Positive Examples\\n:- begin_in_pos.\\n\"\n",
    "    progol_program += \"\\n\".join(positive_examples) + \"\\n:- end_in_pos.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Negative Examples\\n:- begin_in_neg.\\n\"\n",
    "    progol_program += \"\\n\".join(negative_examples) + \"\\n:- end_in_neg.\\n\"\n",
    "\n",
    "    # Save the Progol logic program to a file with a custom name\n",
    "    output_file_path = f\"{output_file_name}.pl\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        file.write(progol_program)\n",
    "\n",
    "    print(f\"Progol logic program saved as {output_file_path}\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "file_path = 'ratings_sample_dataset.xlsx'  # Replace with your actual file path\n",
    "output_dir = './prolog_partitions'  # Directory to save the partitions\n",
    "generate_balanced_partitions(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is encoded in here and working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_1.pl\n",
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_2.pl\n",
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_3.pl\n",
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_4.pl\n",
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_5.pl\n",
      "Progol logic program saved as ./prolog_partitions_six/prolog_program_partition_6.pl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_balanced_partitions(file_path, output_dir):\n",
    "    # Load the dataset\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Define all possible categories for age, genres, and gender\n",
    "    possible_ages = {\n",
    "        \"Under 18\": \"under_18\",\n",
    "        \"18-24\": \"b18to24\",\n",
    "        \"25-34\": \"b25to34\",\n",
    "        \"35-44\": \"b35to44\",\n",
    "        \"45-49\": \"b45to49\",\n",
    "        \"50-55\": \"b50to55\",\n",
    "        \"56+\": \"plus56\"\n",
    "    }\n",
    "    \n",
    "    possible_genres = {\n",
    "        \"Action\": \"action\",\n",
    "        \"Adventure\": \"adventure\",\n",
    "        \"Animation\": \"animation\",\n",
    "        \"Children's\": \"childrens\",\n",
    "        \"Comedy\": \"comedy\",\n",
    "        \"Crime\": \"crime\",\n",
    "        \"Documentary\": \"documentary\",\n",
    "        \"Drama\": \"drama\",\n",
    "        \"Fantasy\": \"fantasy\",\n",
    "        \"Film-Noir\": \"filmnoir\",\n",
    "        \"Horror\": \"horror\",\n",
    "        \"Musical\": \"musical\",\n",
    "        \"Mystery\": \"mystery\",\n",
    "        \"Romance\": \"romance\",\n",
    "        \"Sci-Fi\": \"sci_fi\",\n",
    "        \"Thriller\": \"thriller\",\n",
    "        \"Western\": \"western\",\n",
    "        \"War\": \"war\"\n",
    "    }\n",
    "\n",
    "    possible_genders = [\"m\", \"f\"]\n",
    "\n",
    "    # Split the data into positive and negative examples\n",
    "    positive_data = data[data['rating'] > 3].copy()\n",
    "    negative_data = data[data['rating'] <= 3].copy()\n",
    "\n",
    "    # Shuffle the data\n",
    "    positive_data = positive_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    negative_data = negative_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Determine the size of each partition\n",
    "    num_partitions = 6\n",
    "    pos_partition_size = len(positive_data) // num_partitions\n",
    "    neg_partition_size = len(negative_data) // num_partitions\n",
    "\n",
    "    # Ensure that the partitions are balanced\n",
    "    partitions = []\n",
    "    for i in range(num_partitions):\n",
    "        pos_start = i * pos_partition_size\n",
    "        pos_end = pos_start + pos_partition_size\n",
    "        neg_start = i * neg_partition_size\n",
    "        neg_end = neg_start + neg_partition_size\n",
    "\n",
    "        # Handle remainders by distributing them to the partitions\n",
    "        if i == num_partitions - 1:\n",
    "            pos_end = len(positive_data)\n",
    "            neg_end = len(negative_data)\n",
    "\n",
    "        partition = pd.concat([\n",
    "            positive_data.iloc[pos_start:pos_end],\n",
    "            negative_data.iloc[neg_start:neg_end]\n",
    "        ]).reset_index(drop=True)\n",
    "        \n",
    "        partitions.append(partition)\n",
    "\n",
    "    # Generate Prolog programs for each partition\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for i, partition in enumerate(partitions):\n",
    "        output_file_name = f\"{output_dir}/prolog_program_partition_{i+1}\"\n",
    "        generate_progol_program_for_partition(partition, possible_ages, possible_genres, possible_genders, output_file_name)\n",
    "\n",
    "def generate_progol_program_for_partition(partition, possible_ages, possible_genres, possible_genders, output_file_name):\n",
    "    # Prepare containers for Prolog facts\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "    background_knowledge = set()\n",
    "\n",
    "    # Track the present predicates\n",
    "    present_ages = set()\n",
    "    present_genres = set()\n",
    "    present_genders = set()\n",
    "\n",
    "    # Process each row in the partition\n",
    "    for _, row in partition.iterrows():\n",
    "        user_id = f\"u{row['userid']}\"\n",
    "        movie_id = f\"m{row['productid']}\"\n",
    "        rating = row['rating']\n",
    "        age_group = possible_ages.get(row['age'], \"\").lower()  # Replace age group with corresponding Prolog predicate\n",
    "        gender = row['gender'].lower()  # Normalize gender\n",
    "        genre = possible_genres.get(row['pgenre'], \"\").lower()  # Replace genre with corresponding Prolog predicate\n",
    "\n",
    "        # Generate positive and negative examples\n",
    "        if rating > 3:\n",
    "            positive_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "        else:\n",
    "            negative_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "\n",
    "        # Background knowledge: user attributes\n",
    "        if age_group:\n",
    "            background_knowledge.add(f\"{age_group}({user_id}).\")\n",
    "            present_ages.add(age_group)\n",
    "        if gender in possible_genders:\n",
    "            background_knowledge.add(f\"{gender}({user_id}).\")\n",
    "            present_genders.add(gender)\n",
    "\n",
    "        # Background knowledge: movie genre\n",
    "        if genre:\n",
    "            background_knowledge.add(f\"{genre}({movie_id}).\")\n",
    "            present_genres.add(genre)\n",
    "\n",
    "    # Mode declarations\n",
    "    modeh_declaration = \"modeh(*, recommend(+user, +movie)).\"\n",
    "    modeb_declarations = [\n",
    "        f\"modeb(*, {age}(+user)).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"modeb(*, {gender}(+user)).\" for gender in present_genders\n",
    "    ] + [\n",
    "        f\"modeb(*, {genre}(+movie)).\" for genre in present_genres\n",
    "    ]\n",
    "    modeb_declarations = [declaration for declaration in modeb_declarations if declaration]  # Remove empty strings\n",
    "\n",
    "    # Determinations\n",
    "    determinations = [\n",
    "        f\"determination(recommend/2, {age}/1).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, {gender}/1).\" for gender in present_genders\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, {genre}/1).\" for genre in present_genres\n",
    "    ]\n",
    "    determinations = [determination for determination in determinations if determination]  # Remove empty strings\n",
    "\n",
    "    # Combine all parts into a Progol-compatible logic program\n",
    "    progol_program = \"% Mode Declarations\\n\"\n",
    "    progol_program += modeh_declaration + \"\\n\"\n",
    "    progol_program += \"\\n\".join(modeb_declarations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Determinations\\n\"\n",
    "    progol_program += \"\\n\".join(determinations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Background Knowledge\\n:- begin_bg.\\n\"\n",
    "    progol_program += \"\\n\".join(sorted(background_knowledge)) + \"\\n:- end_bg.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Positive Examples\\n:- begin_in_pos.\\n\"\n",
    "    progol_program += \"\\n\".join(positive_examples) + \"\\n:- end_in_pos.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Negative Examples\\n:- begin_in_neg.\\n\"\n",
    "    progol_program += \"\\n\".join(negative_examples) + \"\\n:- end_in_neg.\\n\"\n",
    "\n",
    "    # Save the Progol logic program to a file with a custom name\n",
    "    output_file_path = f\"{output_file_name}.pl\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        file.write(progol_program)\n",
    "\n",
    "    print(f\"Progol logic program saved as {output_file_path}\")\n",
    "\n",
    "# Usage example:\n",
    "file_path = 'ratings_sample_dataset.xlsx'  # Replace with your actual file path\n",
    "output_dir = './prolog_partitions_six'  # Directory to save the partitions\n",
    "generate_balanced_partitions(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from andante.program import AndanteProgram \n",
    "apmovies = AndanteProgram.build_from(\"prolog_partitions_six/prolog_program_partition_1.pl\")\n",
    "H = apmovies.induce(update_knowledge=True, logging=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedSet([])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine rules from different partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prolog_partitions_six/prolog_program_partition_1.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_2.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_3.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_4.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_5.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_6.pl...\n",
      "recommend(A, B) :- b25to34(A), comedy(B).\n",
      "recommend(A, B) :- crime(B).\n",
      "recommend(A, B) :- f(A).\n",
      "recommend(A, B) :- b25to34(A), action(B).\n",
      "recommend(A, B) :- b45to49(A).\n",
      "recommend(A, B) :- b18to24(A), drama(B).\n",
      "recommend(A, B) :- horror(B).\n",
      "recommend(A, B) :- b25to34(A), f(A).\n",
      "recommend(A, B) :- adventure(B).\n",
      "recommend(A, B) :- m(A), drama(B).\n",
      "Combined rules saved to combined_rules.txt\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from andante.program import AndanteProgram\n",
    "from andante.collections import OrderedSet\n",
    "from andante.logic_concepts import Clause\n",
    "\n",
    "from andante.knowledge import TreeShapedKnowledge\n",
    "# Define the directory containing the Prolog partition files\n",
    "prolog_directory = \"prolog_partitions_six\"\n",
    "\n",
    "# List of partition file names\n",
    "partition_files = [\n",
    "    f\"{prolog_directory}/prolog_program_partition_{i+1}.pl\"\n",
    "    for i in range(6)  # Assuming 6 partitions, adjust as needed\n",
    "]\n",
    "\n",
    "# Initialize an OrderedSet to hold all unique rules\n",
    "all_rules = OrderedSet()\n",
    "\n",
    "# Iterate over each partition file and induce rules\n",
    "for partition_file in partition_files:\n",
    "    print(f\"Processing {partition_file}...\")\n",
    "    # Build the AndanteProgram from the current partition file\n",
    "    ap = AndanteProgram.build_from(partition_file)\n",
    "    \n",
    "    # Induce rules and update knowledge\n",
    "    induced_knowledge = ap.induce(update_knowledge=True, logging=True, verbose=0)\n",
    "    \n",
    "    # If induced_knowledge is a TreeShapedKnowledge, extract its clauses\n",
    "    if isinstance(induced_knowledge, TreeShapedKnowledge):\n",
    "        for clause in induced_knowledge.clauses:\n",
    "            if isinstance(clause, Clause):\n",
    "                all_rules.add(clause)\n",
    "    else:\n",
    "        print(f\"Unexpected type for induced_rules: {type(induced_knowledge)}\")\n",
    "\n",
    "# Output the combined rules\n",
    "for rule in all_rules:\n",
    "    print(rule)\n",
    "\n",
    "# Optionally, save the combined rules to a file\n",
    "with open(\"combined_rules.txt\", \"w\") as f:\n",
    "    for rule in all_rules:\n",
    "        f.write(str(rule) + \"\\n\")\n",
    "\n",
    "print(\"Combined rules saved to combined_rules.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Union with normalization and unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prolog_partitions_six/prolog_program_partition_1.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_2.pl...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Predicate' object has no attribute 'predicate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m induced_knowledge\u001b[38;5;241m.\u001b[39mclauses:\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clause, Clause):\n\u001b[1;32m---> 66\u001b[0m             normalized_clause \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_clause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclause\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m             all_rules\u001b[38;5;241m.\u001b[39madd(normalized_clause)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[136], line 38\u001b[0m, in \u001b[0;36mnormalize_clause\u001b[1;34m(clause)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m             new_terms\u001b[38;5;241m.\u001b[39mappend(term)\n\u001b[1;32m---> 38\u001b[0m     new_body\u001b[38;5;241m.\u001b[39mappend(Atom(\u001b[43matom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredicate\u001b[49m, new_terms))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Apply the same mapping to the head of the clause\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clause\u001b[38;5;241m.\u001b[39mhead:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Predicate' object has no attribute 'predicate'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from andante.program import AndanteProgram\n",
    "from andante.collections import OrderedSet\n",
    "from andante.logic_concepts import Clause, Atom, Variable, Predicate\n",
    "from andante.knowledge import TreeShapedKnowledge\n",
    "\n",
    "# Define the directory containing the Prolog partition files\n",
    "prolog_directory = \"prolog_partitions_six\"\n",
    "\n",
    "# List of partition file names\n",
    "partition_files = [\n",
    "    f\"{prolog_directory}/prolog_program_partition_{i+1}.pl\"\n",
    "    for i in range(6)  # Assuming 6 partitions, adjust as needed\n",
    "]\n",
    "\n",
    "# Initialize an OrderedSet to hold all unique rules\n",
    "all_rules = OrderedSet()\n",
    "\n",
    "# Function to normalize and unify clauses\n",
    "def normalize_clause(clause):\n",
    "    # Sort the literals in the body of the clause for consistent ordering\n",
    "    sorted_body = sorted(clause.body, key=lambda atom: str(atom))\n",
    "\n",
    "    # Standardize variable names: use a consistent naming scheme, e.g., A, B, C...\n",
    "    var_mapping = {}\n",
    "    new_body = []\n",
    "    new_head = clause.head\n",
    "\n",
    "    for atom in sorted_body:\n",
    "        new_terms = []\n",
    "        for term in atom:\n",
    "            if isinstance(term, Variable):\n",
    "                if term not in var_mapping:\n",
    "                    var_mapping[term] = Variable(chr(ord('A') + len(var_mapping)))\n",
    "                new_terms.append(var_mapping[term])\n",
    "            else:\n",
    "                new_terms.append(term)\n",
    "        new_body.append(Atom(atom.predicate, new_terms))\n",
    "\n",
    "    # Apply the same mapping to the head of the clause\n",
    "    if clause.head:\n",
    "        new_head_terms = []\n",
    "        for term in clause.head.terms:\n",
    "            if isinstance(term, Variable):\n",
    "                new_head_terms.append(var_mapping.get(term, term))\n",
    "            else:\n",
    "                new_head_terms.append(term)\n",
    "        new_head = Atom(clause.head.predicate, new_head_terms)\n",
    "\n",
    "    # Return the normalized clause\n",
    "    return Clause(new_head, new_body)\n",
    "\n",
    "# Iterate over each partition file and induce rules\n",
    "for partition_file in partition_files:\n",
    "    print(f\"Processing {partition_file}...\")\n",
    "    # Build the AndanteProgram from the current partition file\n",
    "    ap = AndanteProgram.build_from(partition_file)\n",
    "    \n",
    "    # Induce rules and update knowledge\n",
    "    induced_knowledge = ap.induce(update_knowledge=True, logging=True, verbose=0)\n",
    "    \n",
    "    # If induced_knowledge is a TreeShapedKnowledge, extract its clauses\n",
    "    if isinstance(induced_knowledge, TreeShapedKnowledge):\n",
    "        for clause in induced_knowledge.clauses:\n",
    "            if isinstance(clause, Clause):\n",
    "                normalized_clause = normalize_clause(clause)\n",
    "                all_rules.add(normalized_clause)\n",
    "    else:\n",
    "        print(f\"Unexpected type for induced_rules: {type(induced_knowledge)}\")\n",
    "\n",
    "# Function to check for redundancy and remove duplicates\n",
    "def remove_redundancy(rules):\n",
    "    unique_rules = OrderedSet()\n",
    "    for rule in rules:\n",
    "        if rule not in unique_rules:\n",
    "            unique_rules.add(rule)\n",
    "    return unique_rules\n",
    "\n",
    "# Remove redundancy from all_rules\n",
    "all_rules = remove_redundancy(all_rules)\n",
    "\n",
    "# Output the combined, normalized, and unique rules\n",
    "for rule in all_rules:\n",
    "    print(rule)\n",
    "\n",
    "# Optionally, save the combined rules to a file\n",
    "with open(\"combined_rules.txt\", \"w\") as f:\n",
    "    for rule in all_rules:\n",
    "        f.write(str(rule) + \"\\n\")\n",
    "\n",
    "print(\"Combined, normalized, and unique rules saved to combined_rules_normalized.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prolog_partitions_six/prolog_program_partition_1.pl...\n",
      "Processing prolog_partitions_six/prolog_program_partition_2.pl...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Predicate' object has no attribute 'terms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m clause \u001b[38;5;129;01min\u001b[39;00m induced_knowledge\u001b[38;5;241m.\u001b[39mclauses:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clause, Clause):\n\u001b[1;32m---> 77\u001b[0m             normalized_clause \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_clause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclause\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m             all_rules\u001b[38;5;241m.\u001b[39madd(normalized_clause)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[135], line 39\u001b[0m, in \u001b[0;36mnormalize_clause\u001b[1;34m(clause, var_start)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atom \u001b[38;5;129;01min\u001b[39;00m sorted_body:\n\u001b[0;32m     38\u001b[0m     new_terms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[43matom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterms\u001b[49m:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(term, Variable):\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m var_mapping:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Predicate' object has no attribute 'terms'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from andante.program import AndanteProgram\n",
    "from andante.collections import OrderedSet\n",
    "from andante.logic_concepts import Clause, Atom, Variable\n",
    "from andante.knowledge import TreeShapedKnowledge\n",
    "\n",
    "# Define the directory containing the Prolog partition files\n",
    "prolog_directory = \"prolog_partitions_six\"\n",
    "\n",
    "# List of partition file names\n",
    "partition_files = [\n",
    "    f\"{prolog_directory}/prolog_program_partition_{i+1}.pl\"\n",
    "    for i in range(6)  # Adjust the number of partitions as needed\n",
    "]\n",
    "\n",
    "# Initialize an OrderedSet to hold all unique rules\n",
    "all_rules = OrderedSet()\n",
    "\n",
    "# Function to clean the predicate's string representation\n",
    "def clean_predicate_str(pred):\n",
    "    pred_str = str(pred)\n",
    "    # Assuming the unwanted format is `/1/1`, we can remove it\n",
    "    cleaned_str = pred_str.replace('/1/1', '')\n",
    "    return cleaned_str\n",
    "\n",
    "# Function to normalize and unify clauses\n",
    "def normalize_clause(clause, var_start='A'):\n",
    "    # Sort the literals in the body of the clause for consistent ordering\n",
    "    sorted_body = sorted(clause.body, key=lambda atom: str(atom))\n",
    "\n",
    "    # Standardize variable names: use a consistent naming scheme (e.g., A, B, C...)\n",
    "    var_mapping = {}\n",
    "    var_count = ord(var_start)\n",
    "    new_body = []\n",
    "    new_head = clause.head\n",
    "\n",
    "    for atom in sorted_body:\n",
    "        new_terms = []\n",
    "        for term in atom.terms:\n",
    "            if isinstance(term, Variable):\n",
    "                if term not in var_mapping:\n",
    "                    var_mapping[term] = Variable(chr(var_count))\n",
    "                    var_count += 1\n",
    "                new_terms.append(var_mapping[term])\n",
    "            else:\n",
    "                new_terms.append(term)\n",
    "        # Create a new Atom with the cleaned predicate\n",
    "        cleaned_predicate = clean_predicate_str(atom.predicate)\n",
    "        new_body.append(Atom(cleaned_predicate, new_terms))\n",
    "\n",
    "    # Apply the same mapping to the head of the clause\n",
    "    if clause.head:\n",
    "        new_head_terms = []\n",
    "        for term in clause.head.terms:\n",
    "            if isinstance(term, Variable):\n",
    "                new_head_terms.append(var_mapping.get(term, term))\n",
    "            else:\n",
    "                new_head_terms.append(term)\n",
    "        new_head = Atom(clean_predicate_str(clause.head.predicate), new_head_terms)\n",
    "\n",
    "    # Return the normalized clause with cleaned-up predicates\n",
    "    return Clause(new_head, new_body)\n",
    "\n",
    "# Iterate over each partition file and induce rules\n",
    "for partition_file in partition_files:\n",
    "    print(f\"Processing {partition_file}...\")\n",
    "    # Build the AndanteProgram from the current partition file\n",
    "    ap = AndanteProgram.build_from(partition_file)\n",
    "    \n",
    "    # Induce rules and update knowledge\n",
    "    induced_knowledge = ap.induce(update_knowledge=True, logging=True, verbose=0)\n",
    "    \n",
    "    # If induced_knowledge is a TreeShapedKnowledge, extract its clauses\n",
    "    if isinstance(induced_knowledge, TreeShapedKnowledge):\n",
    "        for clause in induced_knowledge.clauses:\n",
    "            if isinstance(clause, Clause):\n",
    "                normalized_clause = normalize_clause(clause)\n",
    "                all_rules.add(normalized_clause)\n",
    "    else:\n",
    "        print(f\"Unexpected type for induced_rules: {type(induced_knowledge)}\")\n",
    "\n",
    "# Function to check for redundancy and remove duplicates\n",
    "def remove_redundancy(rules):\n",
    "    unique_rules = OrderedSet()\n",
    "    for rule in rules:\n",
    "        normalized_rule = normalize_clause(rule)  # Normalize each rule\n",
    "        if normalized_rule not in unique_rules:\n",
    "            unique_rules.add(normalized_rule)\n",
    "    return unique_rules\n",
    "\n",
    "# Remove redundancy from all_rules and apply final normalization\n",
    "all_rules = remove_redundancy(all_rules)\n",
    "\n",
    "# Output the combined, normalized, and unique rules\n",
    "for rule in all_rules:\n",
    "    print(rule)\n",
    "\n",
    "# Optionally, save the combined rules to a file\n",
    "with open(\"combined_rules.txt\", \"w\") as f:\n",
    "    for rule in all_rules:\n",
    "        f.write(str(rule) + \"\\n\")\n",
    "\n",
    "print(\"Combined, normalized, and unique rules saved to combined_rules.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with the encoding of the NOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_1.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_2.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_3.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_4.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_5.pl\n",
      "Progol logic program saved as ./prolog_partitions/prolog_program_partition_6.pl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_balanced_partitions(file_path, output_dir):\n",
    "    # Load the dataset\n",
    "    data = pd.read_excel(file_path)\n",
    "\n",
    "    # Define all possible categories for age, genres, and gender\n",
    "    possible_ages = {\n",
    "        \"Under 18\": \"under_18\",\n",
    "        \"18-24\": \"b18to24\",\n",
    "        \"25-34\": \"b25to34\",\n",
    "        \"35-44\": \"b35to44\",\n",
    "        \"45-49\": \"b45to49\",\n",
    "        \"50-55\": \"b50to55\",\n",
    "        \"56+\": \"plus56\"\n",
    "    }\n",
    "    \n",
    "    possible_genres = {\n",
    "        \"Action\": \"action\",\n",
    "        \"Adventure\": \"adventure\",\n",
    "        \"Animation\": \"animation\",\n",
    "        \"Children's\": \"childrens\",\n",
    "        \"Comedy\": \"comedy\",\n",
    "        \"Crime\": \"crime\",\n",
    "        \"Documentary\": \"documentary\",\n",
    "        \"Drama\": \"drama\",\n",
    "        \"Fantasy\": \"fantasy\",\n",
    "        \"Film-Noir\": \"filmnoir\",\n",
    "        \"Horror\": \"horror\",\n",
    "        \"Musical\": \"musical\",\n",
    "        \"Mystery\": \"mystery\",\n",
    "        \"Romance\": \"romance\",\n",
    "        \"Sci-Fi\": \"sci_fi\",\n",
    "        \"Thriller\": \"thriller\",\n",
    "        \"Western\": \"western\",\n",
    "        \"War\": \"war\"\n",
    "    }\n",
    "\n",
    "    possible_genders = [\"m\", \"f\"]\n",
    "\n",
    "    # Split the data into positive and negative examples\n",
    "    positive_data = data[data['rating'] > 3].copy()\n",
    "    negative_data = data[data['rating'] <= 3].copy()\n",
    "\n",
    "    # Shuffle the data\n",
    "    positive_data = positive_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    negative_data = negative_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Determine the size of each partition\n",
    "    num_partitions = 6\n",
    "    pos_partition_size = len(positive_data) // num_partitions\n",
    "    neg_partition_size = len(negative_data) // num_partitions\n",
    "\n",
    "    # Ensure that the partitions are balanced\n",
    "    partitions = []\n",
    "    for i in range(num_partitions):\n",
    "        pos_start = i * pos_partition_size\n",
    "        pos_end = pos_start + pos_partition_size\n",
    "        neg_start = i * neg_partition_size\n",
    "        neg_end = neg_start + neg_partition_size\n",
    "\n",
    "        # Handle remainders by distributing them to the partitions\n",
    "        if i == num_partitions - 1:\n",
    "            pos_end = len(positive_data)\n",
    "            neg_end = len(negative_data)\n",
    "\n",
    "        partition = pd.concat([\n",
    "            positive_data.iloc[pos_start:pos_end],\n",
    "            negative_data.iloc[neg_start:neg_end]\n",
    "        ]).reset_index(drop=True)\n",
    "        \n",
    "        partitions.append(partition)\n",
    "\n",
    "    # Generate Prolog programs for each partition\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for i, partition in enumerate(partitions):\n",
    "        output_file_name = f\"{output_dir}/prolog_program_partition_{i+1}\"\n",
    "        generate_progol_program_for_partition(partition, possible_ages, possible_genres, possible_genders, output_file_name)\n",
    "\n",
    "def generate_progol_program_for_partition(partition, possible_ages, possible_genres, possible_genders, output_file_name):\n",
    "    # Prepare containers for Prolog facts\n",
    "    positive_examples = []\n",
    "    negative_examples = []\n",
    "    background_knowledge = set()\n",
    "\n",
    "    # Track the present predicates\n",
    "    present_ages = set()\n",
    "    present_genres = set()\n",
    "    present_genders = set()\n",
    "\n",
    "    # Process each row in the partition\n",
    "    for _, row in partition.iterrows():\n",
    "        user_id = f\"u{row['userid']}\"\n",
    "        movie_id = f\"m{row['productid']}\"\n",
    "        rating = row['rating']\n",
    "        age_group = possible_ages.get(row['age'], \"\").lower()  # Replace age group with corresponding Prolog predicate\n",
    "        gender = row['gender'].lower()  # Normalize gender\n",
    "        genre = possible_genres.get(row['pgenre'], \"\").lower()  # Replace genre with corresponding Prolog predicate\n",
    "\n",
    "        # Generate positive and negative examples\n",
    "        if rating > 3:\n",
    "            positive_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "        else:\n",
    "            negative_examples.append(f\"recommend({user_id}, {movie_id}).\")\n",
    "\n",
    "        # Background knowledge: user attributes (age)\n",
    "        if age_group:\n",
    "            background_knowledge.add(f\"{age_group}({user_id}).\")\n",
    "            present_ages.add(age_group)\n",
    "\n",
    "        # Background knowledge: user attributes (gender)\n",
    "        if gender in possible_genders:\n",
    "            background_knowledge.add(f\"{gender}({user_id}).\")\n",
    "            present_genders.add(gender)\n",
    "\n",
    "        # Background knowledge: movie genre\n",
    "        if genre:\n",
    "            background_knowledge.add(f\"{genre}({movie_id}).\")\n",
    "            present_genres.add(genre)\n",
    "\n",
    "    # Add `not_age` and `not_genre` facts for present categories only\n",
    "    for age_group in present_ages:\n",
    "        user_ids = {f\"u{row['userid']}\" for _, row in partition.iterrows() if possible_ages.get(row['age'], \"\").lower() == age_group}\n",
    "        for user_id in user_ids:\n",
    "            for other_age in possible_ages.values():\n",
    "                if other_age != age_group:\n",
    "                    background_knowledge.add(f\"not_{other_age}({user_id}).\")\n",
    "\n",
    "    for genre in present_genres:\n",
    "        movie_ids = {f\"m{row['productid']}\" for _, row in partition.iterrows() if possible_genres.get(row['pgenre'], \"\").lower() == genre}\n",
    "        for movie_id in movie_ids:\n",
    "            for other_genre in possible_genres.values():\n",
    "                if other_genre != genre:\n",
    "                    background_knowledge.add(f\"not_{other_genre}({movie_id}).\")\n",
    "\n",
    "    # Mode declarations\n",
    "    modeh_declaration = \"modeh(*, recommend(+user, +movie)).\"\n",
    "    modeb_declarations = [\n",
    "        f\"modeb(*, {age}(+user)).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"modeb(*, {gender}(+user)).\" for gender in present_genders\n",
    "    ] + [\n",
    "        f\"modeb(*, {genre}(+movie)).\" for genre in present_genres\n",
    "    ] + [\n",
    "        f\"modeb(*, not_{age}(+user)).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"modeb(*, not_{genre}(+movie)).\" for genre in present_genres\n",
    "    ]\n",
    "    modeb_declarations = [declaration for declaration in modeb_declarations if declaration]  # Remove empty strings\n",
    "\n",
    "    # Determinations\n",
    "    determinations = [\n",
    "        f\"determination(recommend/2, {age}/1).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, {gender}/1).\" for gender in present_genders\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, {genre}/1).\" for genre in present_genres\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, not_{age}/1).\" for age in present_ages\n",
    "    ] + [\n",
    "        f\"determination(recommend/2, not_{genre}/1).\" for genre in present_genres\n",
    "    ]\n",
    "    determinations = [determination for determination in determinations if determination]  # Remove empty strings\n",
    "\n",
    "    # Combine all parts into a Progol-compatible logic program\n",
    "    progol_program = \"% Mode Declarations\\n\"\n",
    "    progol_program += modeh_declaration + \"\\n\"\n",
    "    progol_program += \"\\n\".join(modeb_declarations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Determinations\\n\"\n",
    "    progol_program += \"\\n\".join(determinations) + \"\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Background Knowledge\\n:- begin_bg.\\n\"\n",
    "    progol_program += \"\\n\".join(sorted(background_knowledge)) + \"\\n:- end_bg.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Positive Examples\\n:- begin_in_pos.\\n\"\n",
    "    progol_program += \"\\n\".join(positive_examples) + \"\\n:- end_in_pos.\\n\\n\"\n",
    "\n",
    "    progol_program += \"% Negative Examples\\n:- begin_in_neg.\\n\"\n",
    "    progol_program += \"\\n\".join(negative_examples) + \"\\n:- end_in_neg.\\n\"\n",
    "\n",
    "    # Save the Progol logic program to a file with a custom name\n",
    "    output_file_path = f\"{output_file_name}.pl\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        file.write(progol_program)\n",
    "\n",
    "    print(f\"Progol logic program saved as {output_file_path}\")\n",
    "\n",
    "# Usage example:\n",
    "file_path = 'ratings_sample_dataset.xlsx'  # Replace with your actual file path\n",
    "output_dir = './prolog_partitions'  # Directory to save the partitions\n",
    "generate_balanced_partitions(file_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Knowledge object (class: TreeShapedKnowledge)\n",
       "Clauses:"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from andante.program import AndanteProgram \n",
    "apmovies = AndanteProgram.build_from(\"prolog_partitions/prolog_program_partition_3.pl\")\n",
    "apmovies.induce(update_knowledge=True, logging=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
