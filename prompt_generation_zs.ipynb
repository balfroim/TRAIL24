{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.csv_loader import CSVLoader\n",
    "from models.products.product_registry import ProductRegistry\n",
    "from models.products.product_mapping_row import ProductMappingRow\n",
    "from models.products.product_row import ProductRow\n",
    "\n",
    "product_registry = ProductRegistry(CSVLoader(ProductRow).read(), CSVLoader(ProductMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.users.user_registry import UserRegistry\n",
    "from models.users.user_mapping_row import UserMappingRow\n",
    "from models.users.user_row import UserRow\n",
    "\n",
    "user_registry = UserRegistry(CSVLoader(UserRow).read(), CSVLoader(UserMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ratings.rating_registry import RatingRegistry\n",
    "from models.ratings.rating_row import RatingRow\n",
    "\n",
    "rating_registry = RatingRegistry(CSVLoader(RatingRow).read(), user_registry, product_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reco.reco_factory import RecoFactory\n",
    "import os \n",
    "from paths import PATHS\n",
    "\n",
    "    \n",
    "user_recos = dict()\n",
    "for json_file_name in os.listdir(PATHS[\"recommendations\"]):\n",
    "    user_id = int(json_file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    user_reco_path = os.path.join(PATHS[\"recommendations\"], json_file_name)\n",
    "    user_recos[user_id] = RecoFactory.from_file(user_reco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecoPath(nodes=[RecoNode(type='user', entity_id=33), RecoNode(type='product', entity_id=2346), RecoNode(type='user', entity_id=2678), RecoNode(type='product', entity_id=1762)], rels=[RecoRel(in_node=RecoNode(type='user', entity_id=33), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=1762))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_recos[33][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HUGGINGFACEHUB_API_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m HUGGINGFACEHUB_API_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHUGGINGFACEHUB_API_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/trail/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HUGGINGFACEHUB_API_TOKEN'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use custom API model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Iterator\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "import requests\n",
    "\n",
    "class CustomAPIWrapperLLM(LLM):\n",
    "    \"\"\"\n",
    "    A LangChain-compatible LLM that uses a custom FastAPI for text generation.\n",
    "\n",
    "    This class wraps around a custom FastAPI API and makes HTTP requests\n",
    "    to generate text using the provided prompt.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            llm = CustomAPIWrapperLLM(api_url=\"http://localhost:8000\")\n",
    "            result = llm(\"Tell me a story about a knight\")\n",
    "    \"\"\"\n",
    "\n",
    "    api_url: str\n",
    "    \"\"\"The URL of the FastAPI server to send requests to.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Send a request to the FastAPI server for text generation.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. This example does not implement stop words.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            The generated text from the FastAPI API.\n",
    "        \"\"\"\n",
    "        if stop:\n",
    "            raise ValueError(\"Stop words are not implemented in this example.\")\n",
    "\n",
    "        # Send the prompt to the FastAPI server\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/generate\",\n",
    "            json={\"text\": prompt},\n",
    "        )\n",
    "\n",
    "        # Ensure the response is valid\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Error from API: {response.status_code}, {response.text}\")\n",
    "\n",
    "        # Parse and return the generated text\n",
    "        response_json = response.json()\n",
    "        return response_json.get(\"text\", \"\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"\n",
    "        This method would be used for streaming responses, but it's not implemented\n",
    "        in this example since FastAPI does not support streaming directly.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating (not implemented here).\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Yields:\n",
    "            Iterator of GenerationChunk with streamed text chunks.\n",
    "        \"\"\"\n",
    "        # Assuming your API does not support streaming, this method can be skipped\n",
    "        # or you can raise a NotImplementedError if needed.\n",
    "        raise NotImplementedError(\"Streaming is not implemented for this API\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"api_url\": self.api_url,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of LLM.\"\"\"\n",
    "        return \"custom_api_llm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"top_k\": 50,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"repetition_penalty\": 1.03,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# Initialize the LLM with the FastAPI URL\n",
    "llm = CustomAPIWrapperLLM(api_url=\"http://localhost:28080\")\n",
    "\n",
    "template = \"\"\"{context}\n",
    "\n",
    "You are a tooltip explaining to {user} why {product} was recommended to them in a paragraph.\n",
    "Be clear and concise, no need to greet the user.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation.registry_handler import RegistryHandler\n",
    "from recommendation.explainers.llm_explainer import LLMExplainer\n",
    "\n",
    "\n",
    "registry_handler = RegistryHandler(product_registry, user_registry, rating_registry)\n",
    "explainer = LLMExplainer(registry_handler, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = user_recos[33][1]\n",
    "# cot_explainer._prepare_input(path, [\"name\"])\n",
    "continuation, _ = explainer.explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watched(User33, Product2254)\n",
      "rated(User33, Product2254, 2)\n",
      "watched(User1100, Product2254)\n",
      "rated(User1100, Product2254, 4)\n",
      "watched(User1100, Product1359)\n",
      "rated(User1100, Product1359, 5)\n",
      "gender(User33, F)\n",
      "age(User33, 18-24)\n",
      "name(Product2254, \"Omen, The (1976)\")\n",
      "genre(Product2254, Horror)\n",
      "gender(User1100, M)\n",
      "age(User1100, 35-44)\n",
      "name(Product1359, \"Star Wars: Episode IV - A New Hope (1977)\")\n",
      "genre(Product1359, Action)\n",
      "\n",
      "You are a tooltip explaining to User33 why Product1359 was recommended to them in a paragraph.\n",
      "Be clear and concise, no need to greet the user. \n",
      "This movie is recommended because it has been highly rated by other users with similar demographics as you, such as User1100 who also watched and liked \"The Omen\". Since you're both male and within the same age group, we think you'll enjoy this action-packed classic just like he did.\n",
      "\n",
      " Wait, that's incorrect! User33 is actually female. Let me try again.\n",
      "\n",
      "This movie is recommended because it has been highly rated by other users who have shown interest in movies from the same era as \"The Omen\", which you've watched. Specifically, User1100 gave \"A New Hope\" a perfect score after watching \"The Omen\", suggesting that fans of classic horror might appreciate this iconic sci-fi film.\n"
     ]
    }
   ],
   "source": [
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watched(User33, Product2254)\n",
      "rated(User33, Product2254, 2)\n",
      "watched(User1100, Product2254)\n",
      "rated(User1100, Product2254, 4)\n",
      "watched(User1100, Product1359)\n",
      "rated(User1100, Product1359, 5)\n",
      "gender(User33, F)\n",
      "age(User33, 18-24)\n",
      "genre(Product2254, Horror)\n",
      "gender(User1100, M)\n",
      "age(User1100, 35-44)\n",
      "genre(Product1359, Action)\n",
      "\n",
      "You are a tooltip explaining to User33 why Product1359 was recommended to them in a paragraph.\n",
      "Be clear and concise, no need to greet the user. \n",
      "This is based on collaborative filtering.\n",
      "\n",
      "Product1359 was recommended because other users with similar viewing history and ratings as you also enjoyed it. Specifically, User1100 who watched and rated Product2254 similarly to you, gave Product1359 a high rating of 5, suggesting that fans of horror movies like Product2254 may also appreciate action-packed films like Product1359.\n"
     ]
    }
   ],
   "source": [
    "continuation_without_name, _ = explainer.explain(path, [\"name\"])\n",
    "print(continuation_without_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
