{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.csv_loader import CSVLoader\n",
    "from models.products.product_registry import ProductRegistry\n",
    "from models.products.product_mapping_row import ProductMappingRow\n",
    "from models.products.product_row import ProductRow\n",
    "\n",
    "product_registry = ProductRegistry(CSVLoader(ProductRow).read(), CSVLoader(ProductMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.users.user_registry import UserRegistry\n",
    "from models.users.user_mapping_row import UserMappingRow\n",
    "from models.users.user_row import UserRow\n",
    "\n",
    "user_registry = UserRegistry(CSVLoader(UserRow).read(), CSVLoader(UserMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ratings.rating_registry import RatingRegistry\n",
    "from models.ratings.rating_row import RatingRow\n",
    "\n",
    "rating_registry = RatingRegistry(CSVLoader(RatingRow).read(), user_registry, product_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reco.reco_factory import RecoFactory\n",
    "import os \n",
    "from paths import PATHS\n",
    "\n",
    "    \n",
    "user_recos = dict()\n",
    "for json_file_name in os.listdir(PATHS[\"recommendations\"]):\n",
    "    user_id = int(json_file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    user_reco_path = os.path.join(PATHS[\"recommendations\"], json_file_name)\n",
    "    user_recos[user_id] = RecoFactory.from_file(user_reco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recos[33][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"/gpfs/scratch/acad/trail/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "quantized_8bit = False\n",
    "fp16 = True\n",
    "\n",
    "assert not (quantized_8bit and fp16), \"Cannot use both 8-bit quantization and FP16 at the same time\"\n",
    "\n",
    "\n",
    "if fp16:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "elif quantized_8bit:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "# Set up the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=0,  # Use device 0 for initial input; the model is already distributed\n",
    "    max_new_tokens=1500,\n",
    "    temperature=0.1,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "llm = hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use custom API model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Iterator\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "import requests\n",
    "\n",
    "class CustomAPIWrapperLLM(LLM):\n",
    "    \"\"\"\n",
    "    A LangChain-compatible LLM that uses a custom FastAPI for text generation.\n",
    "\n",
    "    This class wraps around a custom FastAPI API and makes HTTP requests\n",
    "    to generate text using the provided prompt.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            llm = CustomAPIWrapperLLM(api_url=\"http://localhost:8000\")\n",
    "            result = llm(\"Tell me a story about a knight\")\n",
    "    \"\"\"\n",
    "\n",
    "    api_url: str\n",
    "    \"\"\"The URL of the FastAPI server to send requests to.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Send a request to the FastAPI server for text generation.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. This example does not implement stop words.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            The generated text from the FastAPI API.\n",
    "        \"\"\"\n",
    "        if stop:\n",
    "            raise ValueError(\"Stop words are not implemented in this example.\")\n",
    "\n",
    "        # Send the prompt to the FastAPI server\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/generate\",\n",
    "            json={\"text\": prompt},\n",
    "        )\n",
    "\n",
    "        # Ensure the response is valid\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Error from API: {response.status_code}, {response.text}\")\n",
    "\n",
    "        # Parse and return the generated text\n",
    "        response_json = response.json()\n",
    "        return response_json.get(\"text\", \"\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"\n",
    "        This method would be used for streaming responses, but it's not implemented\n",
    "        in this example since FastAPI does not support streaming directly.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating (not implemented here).\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Yields:\n",
    "            Iterator of GenerationChunk with streamed text chunks.\n",
    "        \"\"\"\n",
    "        # Assuming your API does not support streaming, this method can be skipped\n",
    "        # or you can raise a NotImplementedError if needed.\n",
    "        raise NotImplementedError(\"Streaming is not implemented for this API\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"api_url\": self.api_url,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of LLM.\"\"\"\n",
    "        return \"custom_api_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the FastAPI URL\n",
    "api_llm = CustomAPIWrapperLLM(api_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=api_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"top_k\": 50,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"repetition_penalty\": 1.03,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "template = \"\"\"{context}\n",
    "\n",
    "You are a tooltip explaining to {user} why {product} was recommended to them in a paragraph.\n",
    "Be clear and concise, no need to greet the user.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation.registry_handler import RegistryHandler\n",
    "from recommendation.explainers.llm_explainer import LLMExplainer\n",
    "\n",
    "\n",
    "registry_handler = RegistryHandler(product_registry, user_registry, rating_registry)\n",
    "explainer = LLMExplainer(registry_handler, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = user_recos[33][1]\n",
    "# cot_explainer._prepare_input(path, [\"name\"])\n",
    "continuation, _ = explainer.explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watched(User33, Product2254)\n",
      "rated(User33, Product2254, 2)\n",
      "watched(User1100, Product2254)\n",
      "rated(User1100, Product2254, 4)\n",
      "watched(User1100, Product1359)\n",
      "rated(User1100, Product1359, 5)\n",
      "gender(User33, F)\n",
      "age(User33, 18-24)\n",
      "name(Product2254, \"Omen, The (1976)\")\n",
      "genre(Product2254, Horror)\n",
      "gender(User1100, M)\n",
      "age(User1100, 35-44)\n",
      "name(Product1359, \"Star Wars: Episode IV - A New Hope (1977)\")\n",
      "genre(Product1359, Action)\n",
      "\n",
      "You are a tooltip explaining to User33 why Product1359 was recommended to them in a paragraph.\n",
      "Be clear and concise, no need to greet the user. \n",
      "This movie is recommended because it has been highly rated by users with similar demographics as you, such as User1100 who is also male and within the same age group of 35-44 years old. Although you haven't watched this specific movie before, your interest in horror movies like \"The Omen\" suggests that you may enjoy action-packed films with a strong narrative, which is characteristic of \"Star Wars: Episode IV - A New Hope\".\n"
     ]
    }
   ],
   "source": [
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuation_without_name, _ = explainer.explain(path, [\"name\"])\n",
    "print(continuation_without_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
