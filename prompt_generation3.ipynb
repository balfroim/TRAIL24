{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.csv_loader import CSVLoader\n",
    "from models.products.product_registry import ProductRegistry\n",
    "from models.products.product_mapping_row import ProductMappingRow\n",
    "from models.products.product_row import ProductRow\n",
    "\n",
    "product_registry = ProductRegistry(CSVLoader(ProductRow).read(), CSVLoader(ProductMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.users.user_registry import UserRegistry\n",
    "from models.users.user_mapping_row import UserMappingRow\n",
    "from models.users.user_row import UserRow\n",
    "\n",
    "user_registry = UserRegistry(CSVLoader(UserRow).read(), CSVLoader(UserMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ratings.rating_registry import RatingRegistry\n",
    "from models.ratings.rating_row import RatingRow\n",
    "\n",
    "rating_registry = RatingRegistry(CSVLoader(RatingRow).read(), user_registry, product_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reco.reco_factory import RecoFactory\n",
    "import os \n",
    "from paths import PATHS\n",
    "\n",
    "    \n",
    "user_recos = dict()\n",
    "for json_file_name in os.listdir(PATHS[\"recommendations\"]):\n",
    "    user_id = int(json_file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    user_reco_path = os.path.join(PATHS[\"recommendations\"], json_file_name)\n",
    "    user_recos[user_id] = RecoFactory.from_file(user_reco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecoPath(nodes=[RecoNode(type='user', entity_id=33), RecoNode(type='product', entity_id=2346), RecoNode(type='user', entity_id=2678), RecoNode(type='product', entity_id=1762)], rels=[RecoRel(in_node=RecoNode(type='user', entity_id=33), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=1762))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_recos[33][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from models.reco.reco_path import RecoPath\n",
    "\n",
    "\n",
    "def generate_facts(path: RecoPath):\n",
    "    facts_txt = \"% Path: \\n\"\n",
    "    for rel in path.rels:\n",
    "        facts_txt += rel.to_facts() + \"\\n\"\n",
    "        user = user_registry.find_by_eid(rel.in_node.entity_id)\n",
    "        product = product_registry.find_by_eid(rel.out_node.entity_id)\n",
    "        facts_txt += rating_registry.find_user_product_rating(user.uid, product.pid).to_facts() + \"\\n\"\n",
    "    facts_txt += \"% Background Knowledge: \\n\"\n",
    "    for node in path.nodes:\n",
    "        if node.type == \"user\":\n",
    "            user = user_registry.find_by_eid(node.entity_id)\n",
    "            facts_txt += user.to_facts() + \"\\n\"\n",
    "        elif node.type == \"product\":\n",
    "            product = product_registry.find_by_eid(node.entity_id)\n",
    "            facts_txt += product.to_facts() + \"\\n\"\n",
    "    return facts_txt\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_facts(user_recos[33][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /gpfs/home/acad/umons-smit/mmanderl/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # answering_chain.input_schema.schema()\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_community.llms import HuggingFaceEndpoint\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# import json\n",
    "# # from langchain_core.llms import OpenAI\n",
    "\n",
    "# reasoning_prompt_template = \"\"\"You are an expert in graph-based recommender systems.\n",
    "# You try to follow the following explanation goal:\n",
    "# 1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
    "# 2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
    "# 3. **Trust:** Build user’s confidence in the recommender system.\n",
    "# 4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
    "# 5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
    "# 6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
    "# 7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
    "# Given the background knowledge: {background_knowledge},\n",
    "# explain why the movie {movie} was recommended to the user {user}.\n",
    "# \"\"\"\n",
    "\n",
    "# reasoning_prompt = PromptTemplate.from_template(reasoning_prompt_template)\n",
    "\n",
    "# reasoning_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 1500,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"top_k\": 50,\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"repetition_penalty\": 1.1,\n",
    "#         \"length_penalty\": 0.9,\n",
    "#         \"frequency_penalty\": 1.0,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# answering_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 200,\n",
    "#         \"temperature\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model\n",
    "\n",
    "# # Load the tokenizer\n",
    "# # model_name = \"./Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_name = \"./Mistral-7B-Instruct-v0.2\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# # Set up the text generation pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # device=0,  # Use device 0 for initial input; the model is already distributed\n",
    "#     max_new_tokens=1500,\n",
    "#     temperature=0.1,\n",
    "#     top_k=50,\n",
    "#     top_p=0.9,\n",
    "#     repetition_penalty=1.1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model\n",
    "\n",
    "# # Load the tokenizer\n",
    "# # model_name = \"./Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_name = \"./Mistral-7B-Instruct-v0.2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Initialize the model with empty weights to save memory during setup\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "# # Infer the device map to distribute the model across 4 GPUs, each with 38GiB memory\n",
    "# device_map = infer_auto_device_map(\n",
    "#     model,\n",
    "#     # max_memory={0: \"38GiB\", 1: \"38GiB\", 2: \"38GiB\", 3: \"38GiB\"},\n",
    "#     max_memory={0: \"8GiB\", 1: \"8GiB\", 2: \"8GiB\", 3: \"8GiB\"},\n",
    "#     no_split_module_classes=[\"GPTJBlock\"]\n",
    "# )\n",
    "\n",
    "# # Offload any parts that don't fit in GPU memory to the specified directory\n",
    "# offload_dir = \"/gpfs/scratch/acad/trail/tmp/offload_dir\"\n",
    "# model = dispatch_model(model, device_map=device_map, offload_dir=offload_dir)\n",
    "\n",
    "# # Set up the text generation pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # device=0,  # Use device 0 for initial input; the model is already distributed\n",
    "#     max_new_tokens=1500,\n",
    "#     temperature=0.1,\n",
    "#     top_k=50,\n",
    "#     top_p=0.9,\n",
    "#     repetition_penalty=1.1,\n",
    "# )\n",
    "\n",
    "# # answering_llm = reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms.base import LLM\n",
    "# from typing import Optional, List, Any\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from pydantic import Field\n",
    "\n",
    "# class MultiGPUTransformersLLM(LLM):\n",
    "#     model: Any = Field(...)\n",
    "#     tokenizer: Any = Field(...)\n",
    "#     device: int = Field(default=0)\n",
    "\n",
    "#     def __init__(self, model: Any, tokenizer: Any, device: int = 0):\n",
    "#         \"\"\"\n",
    "#         Initialize the multi-GPU transformers LLM.\n",
    "\n",
    "#         Args:\n",
    "#         - model: The distributed model across multiple GPUs.\n",
    "#         - tokenizer: The tokenizer associated with the model.\n",
    "#         - device: GPU device ID for tokenization (handled by model's device map).\n",
    "#         \"\"\"\n",
    "#         # Set attributes using Pydantic's __init__\n",
    "#         super().__init__(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         \"\"\"Return type of LLM.\"\"\"\n",
    "#         return \"multi_gpu_transformers_llm\"\n",
    "\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         \"\"\"\n",
    "#         Run the multi-GPU model on the given prompt and return the generated text.\n",
    "\n",
    "#         Args:\n",
    "#         - prompt: The input prompt string.\n",
    "#         - stop: Optional list of stop tokens.\n",
    "\n",
    "#         Returns:\n",
    "#         - The generated text response.\n",
    "#         \"\"\"\n",
    "#         # Tokenize the input (do not move it to a specific device)\n",
    "#         inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#         # Generate text with the distributed model (no need to move inputs to a specific GPU)\n",
    "#         output_ids = self.model.generate(inputs[\"input_ids\"], max_new_tokens=150, do_sample=True)\n",
    "\n",
    "#         # Decode the generated output\n",
    "#         generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#         # Optionally apply stop tokens\n",
    "#         if stop:\n",
    "#             for stop_token in stop:\n",
    "#                 generated_text = generated_text.split(stop_token)[0]\n",
    "\n",
    "#         return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_gpu_llm = MultiGPUTransformersLLM(model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_gpu_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a09db7a8034ffebff8826ce4dc0c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model locally\n",
    "# model_name = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"  # Replace with your local model path if you have downloaded it locally\n",
    "# model_name = \"./Mixtral-8x22B-Instruct-v0.1\"\n",
    "# model_name = \"./Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_name = \"./Mistral-7B-Instruct-v0.2\"\n",
    "model_name = \"/gpfs/scratch/acad/trail/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "quantized_8bit = False\n",
    "fp16 = True\n",
    "\n",
    "assert not (quantized_8bit and fp16), \"Cannot use both 8-bit quantization and FP16 at the same time\"\n",
    "\n",
    "# If you've downloaded the model locally, replace with the path to the model directory\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # Set up the text generation pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0,  # Set to -1 for CPU, or specify GPU id if using CUDA\n",
    "#     max_new_tokens=1500,\n",
    "#     temperature=0.1,\n",
    "#     top_k=50,\n",
    "#     top_p=0.9,\n",
    "#     repetition_penalty=1.1,\n",
    "# )\n",
    "\n",
    "if fp16:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "elif quantized_8bit:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "# Set up the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=0,  # Use device 0 for initial input; the model is already distributed\n",
    "    max_new_tokens=1500,\n",
    "    temperature=0.1,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "reasoning_llm = hf\n",
    "answering_llm = hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms.base import LLM\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model\n",
    "# from typing import Optional, List\n",
    "\n",
    "# class CustomTransformersLLM(LLM):\n",
    "#     def __init__(self, model_name: str, device: int = -1):\n",
    "#         \"\"\"\n",
    "#         Initialize the LLM with a pre-trained transformers model.\n",
    "        \n",
    "#         Args:\n",
    "#         - model_name: Path to the pre-trained model (local or from Hugging Face Hub).\n",
    "#         - device: GPU device id. Default is -1 for CPU, 0 for the first GPU, etc.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Load tokenizer\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#         # Initialize the model with empty weights to save memory during setup\n",
    "#         with init_empty_weights():\n",
    "#             model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "#         # Infer the device map to distribute the model across GPUs\n",
    "#         device_map = infer_auto_device_map(\n",
    "#             model,\n",
    "#             max_memory={0: \"8GiB\", 1: \"8GiB\", 2: \"8GiB\", 3: \"8GiB\"},\n",
    "#             no_split_module_classes=[\"GPTJBlock\"]\n",
    "#         )\n",
    "\n",
    "#         # Offload parts of the model to disk if necessary\n",
    "#         offload_dir = \"/gpfs/scratch/acad/trail/tmp/offload_dir\"\n",
    "#         model = dispatch_model(model, device_map=device_map, offload_dir=offload_dir)\n",
    "\n",
    "#         # Set up the text generation pipeline using the distributed model\n",
    "#         self.generator = pipeline(\n",
    "#             \"text-generation\",\n",
    "#             model=model,\n",
    "#             tokenizer=tokenizer,\n",
    "#             max_new_tokens=1500,\n",
    "#             temperature=0.1,\n",
    "#             top_k=50,\n",
    "#             top_p=0.9,\n",
    "#             repetition_penalty=1.1,\n",
    "#         )\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         \"\"\"Return the type of LLM.\"\"\"\n",
    "#         return \"custom_transformers_llm\"\n",
    "\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         \"\"\"\n",
    "#         Generate text from the model based on the prompt.\n",
    "\n",
    "#         Args:\n",
    "#         - prompt: The input prompt string.\n",
    "#         - stop: Optional list of stop tokens to stop generation.\n",
    "\n",
    "#         Returns:\n",
    "#         - The generated text response.\n",
    "#         \"\"\"\n",
    "#         if not isinstance(prompt, str):\n",
    "#             raise ValueError(\"Prompt must be a string\")\n",
    "\n",
    "#         # Generate text using the transformers model pipeline\n",
    "#         generated_outputs = self.generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "#         # Extract the generated text\n",
    "#         generated_text = generated_outputs[0]['generated_text']\n",
    "\n",
    "#         # Optionally apply stop tokens\n",
    "#         if stop:\n",
    "#             for stop_token in stop:\n",
    "#                 generated_text = generated_text.split(stop_token)[0]\n",
    "\n",
    "#         return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_name = \"./Mixtral-8x7B-Instruct-v0.1\"\n",
    "# model_name = \"./Mistral-7B-Instruct-v0.2\"\n",
    "# local_llm = CustomTransformersLLM(model_name=model_name, device=0)  # Specify device 0 for GPU, or -1 for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# reasoning_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 1500,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"top_k\": 50,\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"repetition_penalty\": 1.1\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# answering_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 200,\n",
    "#         \"temperature\": 0.4,\n",
    "#         \"top_k\": 30,\n",
    "#         \"top_p\": 0.8,\n",
    "#         \"repetition_penalty\": 1.05\n",
    "#     },\n",
    "# )\n",
    "\n",
    "reasoning_template = \"\"\"You are an expert in graph-based recommender systems.\n",
    "You try to follow the following explanation goal:\n",
    "1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
    "2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
    "3. **Trust:** Build user’s confidence in the recommender system.\n",
    "4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
    "5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
    "6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
    "7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
    "Given the background knowledge: {background_knowledge},\n",
    "explain why the movie \"{product_name}\" was recommended to the user {user}.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_prompt = PromptTemplate.from_template(reasoning_template)\n",
    "\n",
    "answer_template = \"\"\"Based on the reasoning provided: {reasoning},\n",
    "give a concise and helpful response about why the movie \"{product_name}\" was recommended to the user {user} \n",
    "without repeating the explanation goals.\n",
    "\"\"\"\n",
    "answer_prompt = PromptTemplate.from_template(answer_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_chain = reasoning_prompt | reasoning_llm\n",
    "answering_chain = answer_prompt | answering_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning_chain = reasoning_prompt | multi_gpu_llm\n",
    "# answering_chain = answer_prompt | multi_gpu_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation.explainers.cot_explainer import COTExplainer\n",
    "\n",
    "cot_explainer = COTExplainer(product_registry, user_registry, rating_registry, reasoning_chain, answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "path = user_recos[33][0]\n",
    "continuation, trace = cot_explainer.explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer completion:\n",
      "Based on the reasoning provided: You are an expert in graph-based recommender systems.\n",
      "You try to follow the following explanation goal:\n",
      "1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
      "2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
      "3. **Trust:** Build user’s confidence in the recommender system.\n",
      "4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
      "5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
      "6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
      "7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
      "Given the background knowledge: % Path: \n",
      "watched(User33, Product2346)\n",
      "rated(User33, Product2346, 4)\n",
      "watched(User2678, Product2346)\n",
      "rated(User2678, Product2346, 4)\n",
      "watched(User2678, Product1762)\n",
      "rated(User2678, Product1762, 3)\n",
      "% Background Knowledge: \n",
      "gender(User33, \"F\")\n",
      "age(User33, \"18-24\")\n",
      "name(Product2346, \"Fish Called Wanda, A (1988)\")\n",
      "genre(Product2346, \"Comedy\")\n",
      "gender(User2678, \"F\")\n",
      "age(User2678, \"18-24\")\n",
      "name(Product1762, \"Fast Times at Ridgemont High (1982)\")\n",
      "genre(Product1762, \"Comedy\")\n",
      ",\n",
      "explain why the movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33.\n",
      "The movie \"Fast Times at Ridgemont High (1982)\" is recommended because it has been watched by another user with similar demographics (User2678), who also liked the movie \"A Fish Called Wanda (1988)\", which you have previously rated highly. This suggests that you may enjoy other movies from the same genre (Comedy) that have been enjoyed by users with similar tastes and preferences.\n",
      "\n",
      "This explanation aims to achieve the desired goals as follows:\n",
      "\n",
      "*   Transparency: The explanation clearly states the reason behind the recommendation, highlighting the connection between the user's past behavior and the recommended movie.\n",
      "*   Scrutability: By providing specific details about the recommendation process, the user can understand the reasoning and potentially provide feedback if they disagree.\n",
      "*   Trust: The explanation builds trust by showing that the recommendation is based on the user's own preferences and those of similar users.\n",
      "*   Effectiveness: The explanation helps the user make an informed decision by highlighting the relevant factors that led to the recommendation.\n",
      "*   Efficiency: The explanation is concise and easy to understand, facilitating quicker decision-making.\n",
      "*   Persuasiveness: The explanation aims to convince the user of the relevance of the recommendation by emphasizing the connections between their past behavior and the recommended movie.\n",
      "*   Satisfaction: The explanation enhances the overall experience of the system by providing a clear and understandable justification for the recommendation.,\n",
      "give a concise and helpful response about why the movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33 \n",
      "without repeating the explanation goals.\n",
      "The movie \"Fast Times at Ridgemont High (1982)\" is recommended because it has been watched by another user with similar demographics (User2678), who also liked the movie \"A Fish Called Wanda (1988)\", which you have previously rated highly.\n"
     ]
    }
   ],
   "source": [
    "# print(\"reasoning prompt:\")\n",
    "# print(trace.reasoning_trace.prompt)\n",
    "# print(\"reasoning completion:\")\n",
    "# print(trace.reasoning_trace.completion)\n",
    "# print(\"answer prompt:\")\n",
    "# print(trace.answering_trace.prompt)\n",
    "print(\"answer completion:\")\n",
    "print(trace.answering_trace.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i, path in enumerate(user_recos[33][0]):\n",
    "# # path = user_recos[0][0]\n",
    "# def explain(path):\n",
    "#     bk = generate_facts(path)\n",
    "#     product_eid = path.recommendation[1].entity_id\n",
    "#     product = product_registry.find_by_eid(product_eid)\n",
    "#     user_eid = path.recommendation[0].entity_id\n",
    "#     user = user_registry.find_by_eid(user_eid)\n",
    "#     result =   cot_chain.invoke(\n",
    "#         {\"background_knowledge\": bk, \"product_name\": product.name, \"user\": str(user)},\n",
    "#     )\n",
    "#     print(\"-----------------\")\n",
    "#     print(result)\n",
    "#     # print(result)\n",
    "#     # with open(f\"explanations/exp_{i}.txt\", \"w\") as f:\n",
    "#     #     f.write(llm_chain.run(explain(reco)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = user_recos[33][0]\n",
    "# explain(path)\n",
    "# # await explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import logging\n",
    "# # logging.basicConfig(filename='./error.log', level=logging.DEBUG, \n",
    "# #                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# # logger=logging.getLogger(__name__)\n",
    "\n",
    "# for user_reco in user_recos.values():\n",
    "#     for reco in user_reco:\n",
    "#         # try:\n",
    "#         #     # raise Exception(\"test\")\n",
    "#         #     explain(reco)\n",
    "#         # except Exception as e:\n",
    "#         #     print(e)\n",
    "#         #     logger.error(e)\n",
    "\n",
    "\n",
    "# # FIXME: some generate exception\n",
    "#     # print(user_reco)\n",
    "# # for path in user_recos[0]:\n",
    "#     # explain(path)\n",
    "#     # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
