{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.csv_loader import CSVLoader\n",
    "from models.products.product_registry import ProductRegistry\n",
    "from models.products.product_mapping_row import ProductMappingRow\n",
    "from models.products.product_row import ProductRow\n",
    "\n",
    "product_registry = ProductRegistry(CSVLoader(ProductRow).read(), CSVLoader(ProductMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.users.user_registry import UserRegistry\n",
    "from models.users.user_mapping_row import UserMappingRow\n",
    "from models.users.user_row import UserRow\n",
    "\n",
    "user_registry = UserRegistry(CSVLoader(UserRow).read(), CSVLoader(UserMappingRow).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ratings.rating_registry import RatingRegistry\n",
    "from models.ratings.rating_row import RatingRow\n",
    "\n",
    "rating_registry = RatingRegistry(CSVLoader(RatingRow).read(), user_registry, product_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.reco.reco_factory import RecoFactory\n",
    "import os \n",
    "from paths import PATHS\n",
    "\n",
    "    \n",
    "user_recos = dict()\n",
    "for json_file_name in os.listdir(PATHS[\"recommendations\"]):\n",
    "    user_id = int(json_file_name.split(\"_\")[-1].split(\".\")[0])\n",
    "    user_reco_path = os.path.join(PATHS[\"recommendations\"], json_file_name)\n",
    "    user_recos[user_id] = RecoFactory.from_file(user_reco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecoPath(nodes=[RecoNode(type='user', entity_id=33), RecoNode(type='product', entity_id=2346), RecoNode(type='user', entity_id=2678), RecoNode(type='product', entity_id=1762)], rels=[RecoRel(in_node=RecoNode(type='user', entity_id=33), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=2346)), RecoRel(in_node=RecoNode(type='user', entity_id=2678), relation='watched', out_node=RecoNode(type='product', entity_id=1762))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_recos[33][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from models.reco.reco_path import RecoPath\n",
    "\n",
    "\n",
    "def generate_facts(path: RecoPath):\n",
    "    facts_txt = \"% Path: \\n\"\n",
    "    for rel in path.rels:\n",
    "        facts_txt += rel.to_facts() + \"\\n\"\n",
    "        user = user_registry.find_by_eid(rel.in_node.entity_id)\n",
    "        product = product_registry.find_by_eid(rel.out_node.entity_id)\n",
    "        facts_txt += rating_registry.find_user_product_rating(user.uid, product.pid).to_facts() + \"\\n\"\n",
    "    facts_txt += \"% Background Knowledge: \\n\"\n",
    "    for node in path.nodes:\n",
    "        if node.type == \"user\":\n",
    "            user = user_registry.find_by_eid(node.entity_id)\n",
    "            facts_txt += user.to_facts() + \"\\n\"\n",
    "        elif node.type == \"product\":\n",
    "            product = product_registry.find_by_eid(node.entity_id)\n",
    "            facts_txt += product.to_facts() + \"\\n\"\n",
    "    return facts_txt\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Path: \n",
      "watched(User33, Product2346)\n",
      "rated(User33, Product2346, 4)\n",
      "watched(User2678, Product2346)\n",
      "rated(User2678, Product2346, 4)\n",
      "watched(User2678, Product1762)\n",
      "rated(User2678, Product1762, 3)\n",
      "% Background Knowledge: \n",
      "gender(User33, \"F\")\n",
      "age(User33, \"18-24\")\n",
      "name(Product2346, \"Fish Called Wanda, A (1988)\")\n",
      "genre(Product2346, \"Comedy\")\n",
      "gender(User2678, \"F\")\n",
      "age(User2678, \"18-24\")\n",
      "name(Product1762, \"Fast Times at Ridgemont High (1982)\")\n",
      "genre(Product1762, \"Comedy\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_facts(user_recos[33][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # answering_chain.input_schema.schema()\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_community.llms import HuggingFaceEndpoint\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# import json\n",
    "# # from langchain_core.llms import OpenAI\n",
    "\n",
    "# reasoning_prompt_template = \"\"\"You are an expert in graph-based recommender systems.\n",
    "# You try to follow the following explanation goal:\n",
    "# 1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
    "# 2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
    "# 3. **Trust:** Build user’s confidence in the recommender system.\n",
    "# 4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
    "# 5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
    "# 6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
    "# 7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
    "# Given the background knowledge: {background_knowledge},\n",
    "# explain why the movie {movie} was recommended to the user {user}.\n",
    "# \"\"\"\n",
    "\n",
    "# reasoning_prompt = PromptTemplate.from_template(reasoning_prompt_template)\n",
    "\n",
    "# reasoning_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 1500,\n",
    "#         \"temperature\": 0.1,\n",
    "#         \"top_k\": 50,\n",
    "#         \"top_p\": 0.9,\n",
    "#         \"repetition_penalty\": 1.1,\n",
    "#         \"length_penalty\": 0.9,\n",
    "#         \"frequency_penalty\": 1.0,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# answering_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "#     **{\n",
    "#         \"max_new_tokens\": 200,\n",
    "#         \"temperature\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balfroim/.cache/pypoetry/virtualenvs/trail24-UZVMavSK-py3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from langchain-huggingface package and should be used instead. To use it run `pip install -U from langchain-huggingface` and import as `from from langchain_huggingface import llms import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/balfroim/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/balfroim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "reasoning_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    **{\n",
    "        \"max_new_tokens\": 1500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    },\n",
    ")\n",
    "\n",
    "answering_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    **{\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"temperature\": 0.4,\n",
    "        \"top_k\": 30,\n",
    "        \"top_p\": 0.8,\n",
    "        \"repetition_penalty\": 1.05\n",
    "    },\n",
    ")\n",
    "\n",
    "reasoning_template = \"\"\"You are an expert in graph-based recommender systems.\n",
    "You try to follow the following explanation goal:\n",
    "1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
    "2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
    "3. **Trust:** Build user’s confidence in the recommender system.\n",
    "4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
    "5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
    "6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
    "7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
    "Given the background knowledge: {background_knowledge},\n",
    "explain why the movie \"{product_name}\" was recommended to the user {user}.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_prompt = PromptTemplate.from_template(reasoning_template)\n",
    "\n",
    "answer_template = \"\"\"Based on the reasoning provided: {reasoning},\n",
    "give a concise and helpful response about why the movie \"{product_name}\" was recommended to the user {user} \n",
    "without repeating the explanation goals.\n",
    "\"\"\"\n",
    "answer_prompt = PromptTemplate.from_template(answer_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_chain = reasoning_prompt | reasoning_llm\n",
    "answering_chain = answer_prompt | answering_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommendation.explainers.cot_explainer import COTExplainer\n",
    "\n",
    "cot_explainer = COTExplainer(product_registry, user_registry, rating_registry, reasoning_chain, answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = user_recos[33][0]\n",
    "continuation, trace = cot_explainer.explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning prompt:\n",
      "You are an expert in graph-based recommender systems.\n",
      "You try to follow the following explanation goal:\n",
      "1. **Transparency:** Clearly explain how the recommendation algorithm made the decision.\n",
      "2. **Scrutability:** Allow the user to provide feedback if the recommendation seems incorrect.\n",
      "3. **Trust:** Build user’s confidence in the recommender system.\n",
      "4. **Effectiveness:** Help user make informed decisions about the recommendation.\n",
      "5. **Efficiency:** Provide a quick explanation to facilitate faster decision-making.\n",
      "6. **Persuasiveness:** Convince user of the relevance of the recommendation.\n",
      "7. **Satisfaction:** Enhance the ease of use and overall experience of the system for the user.\n",
      "Given the background knowledge: % Path: \n",
      "watched(User33, Product2346)\n",
      "rated(User33, Product2346, 4)\n",
      "watched(User2678, Product2346)\n",
      "rated(User2678, Product2346, 4)\n",
      "watched(User2678, Product1762)\n",
      "rated(User2678, Product1762, 3)\n",
      "% Background Knowledge: \n",
      "gender(User33, \"F\")\n",
      "age(User33, \"18-24\")\n",
      "name(Product2346, \"Fish Called Wanda, A (1988)\")\n",
      "genre(Product2346, \"Comedy\")\n",
      "gender(User2678, \"F\")\n",
      "age(User2678, \"18-24\")\n",
      "name(Product1762, \"Fast Times at Ridgemont High (1982)\")\n",
      "genre(Product1762, \"Comedy\")\n",
      ",\n",
      "explain why the movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33.\n",
      "\n",
      "reasoning completion:\n",
      "\n",
      "The movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33 based on the collaborative filtering approach. Specifically, we found another user with similar preferences as User33, who has already watched and rated \"Fast Times at Ridgemont High (1982)\". This other user is User2678, who shares the same gender and age range as User33. Furthermore, User2678 also watched and rated another comedy movie, \"A Fish Called Wanda (1988)\", which User33 has already watched and rated highly. Therefore, based on User2678's positive rating of \"Fast Times at Ridgemont High (1982)\", we recommend this movie to User33.\n",
      "\n",
      "This recommendation aims to be transparent by explaining the reasoning behind it, scrutable by allowing User33 to provide feedback or ignore the recommendation, trustworthy by showing that it is based on the preferences of a similar user, effective by providing relevant information about the recommended movie, efficient by providing a quick explanation, persuasive by highlighting the similarity between User33 and User2678, and satisfying by enhancing the user experience.\n",
      "answer prompt:\n",
      "Based on the reasoning provided: \n",
      "The movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33 based on the collaborative filtering approach. Specifically, we found another user with similar preferences as User33, who has already watched and rated \"Fast Times at Ridgemont High (1982)\". This other user is User2678, who shares the same gender and age range as User33. Furthermore, User2678 also watched and rated another comedy movie, \"A Fish Called Wanda (1988)\", which User33 has already watched and rated highly. Therefore, based on User2678's positive rating of \"Fast Times at Ridgemont High (1982)\", we recommend this movie to User33.\n",
      "\n",
      "This recommendation aims to be transparent by explaining the reasoning behind it, scrutable by allowing User33 to provide feedback or ignore the recommendation, trustworthy by showing that it is based on the preferences of a similar user, effective by providing relevant information about the recommended movie, efficient by providing a quick explanation, persuasive by highlighting the similarity between User33 and User2678, and satisfying by enhancing the user experience.,\n",
      "give a concise and helpful response about why the movie \"Fast Times at Ridgemont High (1982)\" was recommended to the user User33 \n",
      "without repeating the explanation goals.\n",
      "\n",
      "answer completion:\n",
      "\n",
      "\"Fast Times at Ridgemont High (1982)\" was recommended to User33 based on the collaborative filtering approach. We found another user, User2678, with similar preferences who rated the movie positively. User2678 is of the same gender and age range as User33 and also rated \"A Fish Called Wanda (1988)\" highly, which User33 has already watched and rated. Therefore, we recommend \"Fast Times at Ridgemont High (1982)\" to User33 based on User2678's positive rating.\n"
     ]
    }
   ],
   "source": [
    "print(\"reasoning prompt:\")\n",
    "print(trace.reasoning_trace.prompt)\n",
    "print(\"reasoning completion:\")\n",
    "print(trace.reasoning_trace.completion)\n",
    "print(\"answer prompt:\")\n",
    "print(trace.answering_trace.prompt)\n",
    "print(\"answer completion:\")\n",
    "print(trace.answering_trace.completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i, path in enumerate(user_recos[33][0]):\n",
    "# # path = user_recos[0][0]\n",
    "# def explain(path):\n",
    "#     bk = generate_facts(path)\n",
    "#     product_eid = path.recommendation[1].entity_id\n",
    "#     product = product_registry.find_by_eid(product_eid)\n",
    "#     user_eid = path.recommendation[0].entity_id\n",
    "#     user = user_registry.find_by_eid(user_eid)\n",
    "#     result =   cot_chain.invoke(\n",
    "#         {\"background_knowledge\": bk, \"product_name\": product.name, \"user\": str(user)},\n",
    "#     )\n",
    "#     print(\"-----------------\")\n",
    "#     print(result)\n",
    "#     # print(result)\n",
    "#     # with open(f\"explanations/exp_{i}.txt\", \"w\") as f:\n",
    "#     #     f.write(llm_chain.run(explain(reco)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = user_recos[33][0]\n",
    "# explain(path)\n",
    "# # await explain(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import logging\n",
    "# # logging.basicConfig(filename='./error.log', level=logging.DEBUG, \n",
    "# #                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# # logger=logging.getLogger(__name__)\n",
    "\n",
    "# for user_reco in user_recos.values():\n",
    "#     for reco in user_reco:\n",
    "#         # try:\n",
    "#         #     # raise Exception(\"test\")\n",
    "#         #     explain(reco)\n",
    "#         # except Exception as e:\n",
    "#         #     print(e)\n",
    "#         #     logger.error(e)\n",
    "\n",
    "\n",
    "# # FIXME: some generate exception\n",
    "#     # print(user_reco)\n",
    "# # for path in user_recos[0]:\n",
    "#     # explain(path)\n",
    "#     # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
